{
  "2409.12922v1": {
    "title": "AI Thinking: A framework for rethinking artificial intelligence in practice",
    "authors": [
      "Denis Newman-Griffis"
    ],
    "summary": "Artificial intelligence is transforming the way we work with information\nacross disciplines and practical contexts. A growing range of disciplines are\nnow involved in studying, developing, and assessing the use of AI in practice,\nbut these disciplines often employ conflicting understandings of what AI is and\nwhat is involved in its use. New, interdisciplinary approaches are needed to\nbridge competing conceptualisations of AI in practice and help shape the future\nof AI use. I propose a novel conceptual framework called AI Thinking, which\nmodels key decisions and considerations involved in AI use across disciplinary\nperspectives. The AI Thinking model addresses five practice-based competencies\ninvolved in applying AI in context: motivating AI use in information processes,\nformulating AI methods, assessing available tools and technologies, selecting\nappropriate data, and situating AI in the sociotechnical contexts it is used\nin. A hypothetical case study is provided to illustrate the application of AI\nThinking in practice. This article situates AI Thinking in broader\ncross-disciplinary discourses of AI, including its connections to ongoing\ndiscussions around AI literacy and AI-driven innovation. AI Thinking can help\nto bridge divides between academic disciplines and diverse contexts of AI use,\nand to reshape the future of AI in practice.",
    "pdf_url": "http://arxiv.org/pdf/2409.12922v1",
    "published": "2024-08-26"
  },
  "2406.11563v3": {
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "authors": [
      "Andr\u00e9 Platzer"
    ],
    "summary": "This perspective piece calls for the study of the new field of Intersymbolic\nAI, by which we mean the combination of symbolic AI, whose building blocks have\ninherent significance/meaning, with subsymbolic AI, whose entirety creates\nsignificance/effect despite the fact that individual building blocks escape\nmeaning. Canonical kinds of symbolic AI are logic, games and planning.\nCanonical kinds of subsymbolic AI are (un)supervised machine and reinforcement\nlearning. Intersymbolic AI interlinks the worlds of symbolic AI with its\ncompositional symbolic significance and meaning and of subsymbolic AI with its\nsummative significance or effect to enable culminations of insights from both\nworlds by going between and across symbolic AI insights with subsymbolic AI\ntechniques that are being helped by symbolic AI principles. For example,\nIntersymbolic AI may start with symbolic AI to understand a dynamic system,\ncontinue with subsymbolic AI to learn its control, and end with symbolic AI to\nsafely use the outcome of the learned subsymbolic AI controller in the dynamic\nsystem. The way Intersymbolic AI combines both symbolic and subsymbolic AI to\nincrease the effectiveness of AI compared to either kind of AI alone is likened\nto the way that the combination of both conscious and subconscious thought\nincreases the effectiveness of human thought compared to either kind of thought\nalone. Some successful contributions to the Intersymbolic AI paradigm are\nsurveyed here but many more are considered possible by advancing Intersymbolic\nAI.",
    "pdf_url": "http://arxiv.org/pdf/2406.11563v3",
    "published": "2024-06-17"
  },
  "2402.07632v3": {
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "authors": [
      "Jingshu Li",
      "Yitian Yang",
      "Renwen Zhang",
      "Yi-chieh Lee"
    ],
    "summary": "AI transparency is a central pillar of responsible AI deployment and\neffective human-AI collaboration. A critical approach is communicating\nuncertainty, such as displaying AI's confidence level, or its correctness\nlikelihood (CL), to users. However, these confidence levels are often\nuncalibrated, either overestimating or underestimating actual CL, posing risks\nand harms to human-AI collaboration. This study examines the effects of\nuncalibrated AI confidence on users' trust in AI, AI advice adoption, and\ncollaboration outcomes. We further examined the impact of increased\ntransparency, achieved through trust calibration support, on these outcomes.\nOur results reveal that uncalibrated AI confidence leads to both the misuse of\noverconfident AI and disuse of unconfident AI, thereby hindering outcomes of\nhuman-AI collaboration. Deficiency of trust calibration support exacerbates\nthis issue by making it harder to detect uncalibrated confidence, promoting\nmisuse and disuse of AI. Conversely, trust calibration support aids in\nrecognizing uncalibration and reducing misuse, but it also fosters distrust and\ncauses disuse of AI. Our findings highlight the importance of AI confidence\ncalibration for enhancing human-AI collaboration and suggest directions for AI\ndesign and regulation.",
    "pdf_url": "http://arxiv.org/pdf/2402.07632v3",
    "published": "2024-02-12"
  },
  "2211.05075v1": {
    "title": "Supporting AI/ML Security Workers through an Adversarial Techniques, Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
    "authors": [
      "Mohamad Fazelnia",
      "Ahmet Okutan",
      "Mehdi Mirakhorli"
    ],
    "summary": "This paper focuses on supporting AI/ML Security Workers -- professionals\ninvolved in the development and deployment of secure AI-enabled software\nsystems. It presents AI/ML Adversarial Techniques, Tools, and Common Knowledge\n(AI/ML ATT&CK) framework to enable AI/ML Security Workers intuitively to\nexplore offensive and defensive tactics.",
    "pdf_url": "http://arxiv.org/pdf/2211.05075v1",
    "published": "2022-11-09"
  },
  "2403.15481v2": {
    "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and Strategies in AI/ML Development",
    "authors": [
      "Aastha Pant",
      "Rashina Hoda",
      "Chakkrit Tantithamthavorn",
      "Burak Turhan"
    ],
    "summary": "The rise in the use of AI/ML applications across industries has sparked more\ndiscussions about the fairness of AI/ML in recent times. While prior research\non the fairness of AI/ML exists, there is a lack of empirical studies focused\non understanding the perspectives and experiences of AI practitioners in\ndeveloping a fair AI/ML system. Understanding AI practitioners' perspectives\nand experiences on the fairness of AI/ML systems are important because they are\ndirectly involved in its development and deployment and their insights can\noffer valuable real-world perspectives on the challenges associated with\nensuring fairness in AI/ML systems. We conducted semi-structured interviews\nwith 22 AI practitioners to investigate their understanding of what a 'fair\nAI/ML' is, the challenges they face in developing a fair AI/ML system, the\nconsequences of developing an unfair AI/ML system, and the strategies they\nemploy to ensure AI/ML system fairness. We developed a framework showcasing the\nrelationship between AI practitioners' understanding of 'fair AI/ML' system and\n(i) their challenges in its development, (ii) the consequences of developing an\nunfair AI/ML system, and (iii) strategies used to ensure AI/ML system fairness.\nBy exploring AI practitioners' perspectives and experiences, this study\nprovides actionable insights to enhance AI/ML fairness, which may promote\nfairer systems, reduce bias, and foster public trust in AI technologies.\nAdditionally, we also identify areas for further investigation and offer\nrecommendations to aid AI practitioners and AI companies in navigating\nfairness.",
    "pdf_url": "http://arxiv.org/pdf/2403.15481v2",
    "published": "2024-03-21"
  }
}